{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b2fef23",
   "metadata": {},
   "source": [
    "To install pytorch geometric run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f7d42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch=='1.9.0'\n",
    "#!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.9.0+cu102.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76e04ce",
   "metadata": {},
   "source": [
    "# IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05d94c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score,average_precision_score\n",
    "\n",
    "from torch_geometric.utils import negative_sampling\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from torch_geometric.transforms import RandomLinkSplit,NormalizeFeatures,Constant,OneHotDegree\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.nn import GCNConv,SAGEConv,GATConv\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import torch\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "import itertools\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb8b40a",
   "metadata": {},
   "source": [
    "# LOAD DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454a1eb1",
   "metadata": {},
   "source": [
    "You have to choose only one of the following four cells to construct the time slices over a certain time interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cea13b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time period used in the works\n",
    "time_slices = {\n",
    "    0: {\n",
    "        'future_start': '2016-07-03',\n",
    "        'future_end': '2016-08-02',\n",
    "        'start' : '2016-06-03',\n",
    "        'end': '2016-07-02'\n",
    "    },\n",
    "    \n",
    "    1: {\n",
    "        'future_start': '2016-08-03',\n",
    "        'future_end': '2016-09-02',\n",
    "        'start': '2016-07-03',\n",
    "        'end': '2016-08-02'\n",
    "    },\n",
    "    \n",
    "    2:{\n",
    "        'future_start': '2016-09-03',\n",
    "        'future_end': '2016-10-02',\n",
    "        'start': '2016-08-03',\n",
    "        'end': '2016-09-02'\n",
    "    },\n",
    "    \n",
    "    3:{\n",
    "        'future_start': '2016-10-03',\n",
    "        'future_end': '2016-11-02',\n",
    "        'start': '2016-09-03',\n",
    "        'end': '2016-10-02'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c49585",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 days time slice\n",
    "time_slices = {\n",
    "    0: {\n",
    "        'future_start': '2016-08-03',\n",
    "        'future_end': '2016-08-12',\n",
    "        'start' : '2016-06-03',\n",
    "        'end': '2016-08-02'\n",
    "    },\n",
    "    \n",
    "    1: {\n",
    "        'future_start': '2016-08-13',\n",
    "        'future_end': '2016-08-22',\n",
    "        'start': '2016-08-03',\n",
    "        'end': '2016-08-12'\n",
    "    },\n",
    "    \n",
    "    2:{\n",
    "        'future_start': '2016-08-23',\n",
    "        'future_end': '2016-09-01',\n",
    "        'start': '2016-08-13',\n",
    "        'end': '2016-08-22'\n",
    "    },\n",
    "    \n",
    "    3:{\n",
    "        'future_start': '2016-09-02',\n",
    "        'future_end': '2016-09-11',\n",
    "        'start': '2016-08-23',\n",
    "        'end': '2016-09-01'\n",
    "    },\n",
    "    \n",
    "    4:{\n",
    "        'future_start': '2016-09-12',\n",
    "        'future_end': '2016-09-21',\n",
    "        'start': '2016-09-02',\n",
    "        'end': '2016-09-11'\n",
    "    },\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f94e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nov-Dec 2016 time interval\n",
    "time_slices = {\n",
    "    0: {\n",
    "        'future_start': '2016-12-03',\n",
    "        'future_end': '2017-01-02',\n",
    "        'start' : '2016-06-03',\n",
    "        'end': '2016-12-02'\n",
    "    },\n",
    "    \n",
    "    1: {\n",
    "        'future_start': '2017-01-03',\n",
    "        'future_end': '2017-02-02',\n",
    "        'start': '2016-12-03',\n",
    "        'end': '2017-01-02'\n",
    "    },\n",
    "    \n",
    "    2:{\n",
    "        'future_start': '2017-02-03',\n",
    "        'future_end': '2017-03-02',\n",
    "        'start': '2017-01-03',\n",
    "        'end': '2017-02-02'\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e839d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nov-Dec 2016 time interval with 10 days time slice\n",
    "time_slices = {\n",
    "    0: {\n",
    "        'future_start': '2016-12-03',\n",
    "        'future_end': '2016-12-12',\n",
    "        'start' : '2016-06-03',\n",
    "        'end': '2016-12-02'\n",
    "    },\n",
    "    \n",
    "    1: {\n",
    "        'future_start': '2016-12-13',\n",
    "        'future_end': '2016-12-22',\n",
    "        'start': '2016-12-03',\n",
    "        'end': '2016-12-12'\n",
    "    },\n",
    "    \n",
    "    2:{\n",
    "        'future_start': '2016-12-23',\n",
    "        'future_end': '2017-01-01',\n",
    "        'start': '2016-12-13',\n",
    "        'end': '2016-12-22'\n",
    "    },\n",
    "    \n",
    "    3:{\n",
    "        'future_start': '2017-01-02',\n",
    "        'future_end': '2017-01-11',\n",
    "        'start': '2016-12-23',\n",
    "        'end': '2017-01-01'\n",
    "    },\n",
    "    4:{\n",
    "        'future_start': '2017-01-12',\n",
    "        'future_end': '2017-01-21',\n",
    "        'start': '2017-01-02',\n",
    "        'end': '2017-01-11'\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0ed7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERIOD = 1\n",
    "\n",
    "start_train = time_slices[0]['start']\n",
    "end_train = time_slices[PERIOD]['end']\n",
    "\n",
    "start_future = time_slices[0]['start']\n",
    "end_future = time_slices[PERIOD]['future_end']\n",
    "\n",
    "start_future_test = time_slices[0]['start']\n",
    "end_future_test = time_slices[PERIOD+1]['future_end']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a2dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpath_train = f\"dataset/graph/custom_json_{start_train}_{end_train}\"\n",
    "#gpath_future = f\"dataset/graph/custom_json_{start_future}_{end_future}\"\n",
    "#gpath_test_future = f\"dataset/graph/custom_json_{start_future_test}_{end_future_test}\"\n",
    "gpath_train = f\"../dummy-data/gnn/custom_json_{start_train}_{end_train}\"\n",
    "gpath_future = f\"../dummy-data/gnn/custom_json_{start_future}_{end_future}\"\n",
    "gpath_test_future = f\"../dummy-data/gnn/custom_json_{start_future_test}_{end_future_test}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4f0b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c7592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#textfpath_train = f\"dataset/textFeatures/textFeatures_{start_train}_{end_train}.json\" \n",
    "#textfpath_test = f\"dataset/textFeatures/textFeatures_{start_future}_{end_future}.json\"\n",
    "textfpath_train = f\"../dummy-data/gnn/textFeatures_{start_train}_{end_train}.json\" \n",
    "textfpath_test = f\"../dummy-data/gnn/textFeatures_{start_future}_{end_future}.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbfefc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pyg_dataset(gpath_current, gpath_future, feature = 'struct', textfpath = None, topicSim=False):\n",
    "    \n",
    "    if feature not in ['constant','one-hot-encode','struct','text','topic','all']:\n",
    "        raise Exception('Invalid feature value')\n",
    "    \n",
    "    #LOAD GRAPH AND COMPUTE NODE FEATURES\n",
    "    G_current_all = nx.read_edgelist(gpath_current, create_using=nx.DiGraph)\n",
    "\n",
    "    if feature == 'struct' or feature == 'all':\n",
    "        #COMPUTE STRUCTURAL FEATURES\n",
    "        pr = nx.pagerank(G_current_all)\n",
    "        indeg = dict(G_current_all.in_degree())\n",
    "        outdeg = dict(G_current_all.out_degree())\n",
    "        neighdeg = nx.average_neighbor_degree(G_current_all)\n",
    "    \n",
    "    if feature == 'text' or feature == 'topic' or feature == 'all':\n",
    "        #COMPUTE TEXTUAL FEATURES\n",
    "        if textfpath is None:\n",
    "            raise Exception('textfpath parameter must be not None if feature is text or all')\n",
    "        with open(textfpath) as f:\n",
    "            textf_raw = json.load(f)\n",
    "            textf = dict()\n",
    "            textf['numPost'] = textf_raw['numPost']\n",
    "            textf['numComment'] = textf_raw['numComment']\n",
    "            textf['numTag'] = textf_raw['numTag']\n",
    "            textf['corpusLength'] = textf_raw['corpusLength']\n",
    "            textf['corpusStdev'] = textf_raw['corpusStdev']\n",
    "            \n",
    "            if feature == 'topic':\n",
    "                topics = {}\n",
    "                for user,lda in textf_raw['lda_all'].items():\n",
    "                    for i in range(30): #30 is the number of LDA topic\n",
    "                        if f'topic_{i}' not in topics:\n",
    "                            topics[f'topic_{i}'] = {}\n",
    "                        topics[f'topic_{i}'][user] = lda[i]\n",
    "                        \n",
    "                for name,topicf in topics.items():\n",
    "                    textf[name] = topicf\n",
    "            \n",
    "            if topicSim: \n",
    "                #compute an index based on entropy of LDA distributions to reflect\n",
    "                #the user importance in term of its contents\n",
    "                #the study on this feature is still not covered on our works\n",
    "                entropy_all = {user:entropy(ldaAll) for user,ldaAll in textf_raw['lda_all'].items()}\n",
    "                #entropy_post = {user:entropy(ldaPost) for user,ldaPost in textf_raw['lda_post'].items()}\n",
    "                #entropy_comment = {user:entropy(ldaComment) for user,ldaComment in textf_raw['lda_comment'].items()}\n",
    "                reciprocal_mean_followers_entropy_all = {}\n",
    "                for user in G_current_all.nodes():\n",
    "                    sum_followers_entropies = 0\n",
    "                    count_followers = 0\n",
    "                    for follower in G_current_all.predecessors(user):\n",
    "                        follower_entropy = entropy_all[follower]\n",
    "                        sum_followers_entropies += follower_entropy\n",
    "                        count_followers += 1\n",
    "                    mean_followers_entropy = 0\n",
    "                    if count_followers > 0:\n",
    "                        mean_followers_entropy = sum_followers_entropies / count_followers\n",
    "                    reciprocal_mean = 0\n",
    "                    if mean_followers_entropy != 0:\n",
    "                        reciprocal_mean = 1 / mean_followers_entropy\n",
    "                    reciprocal_mean_followers_entropy_all[user] = reciprocal_mean\n",
    "                textf['reciprocal_mean_followers_entropy_all'] = reciprocal_mean_followers_entropy_all\n",
    "                    \n",
    "                \n",
    "    #POSITIVE SET WITH LABELLING ON THE FUTURE PERIOD\n",
    "    G_future = nx.read_edgelist(gpath_future, create_using=nx.DiGraph)\n",
    "\n",
    "    G_current = nx.DiGraph(G_future.subgraph(G_current_all.nodes()))\n",
    "    \n",
    "    if feature == 'struct' or feature == 'text' or feature == 'topic' or feature == 'all':\n",
    "        if feature == 'struct' or feature == 'all':\n",
    "            #ASSIGN STRUCTURAL FEATURES TO EACH NODE\n",
    "            nx.set_node_attributes(G_current,pr,'pagerank')\n",
    "            nx.set_node_attributes(G_current,indeg,'in_degree')\n",
    "            nx.set_node_attributes(G_current,outdeg,'out_degree')\n",
    "            nx.set_node_attributes(G_current,neighdeg,'neigh_degree')\n",
    "            #nx.set_node_attributes(G_train,nx.clustering(G_train),'clustering')\n",
    "            #nx.set_node_attributes(G_train,nx.katz_centrality(G_train),'katz')\n",
    "            \n",
    "        if feature == 'text' or feature == 'all' or feature == 'topic':\n",
    "            #ASSIGN TEXTUAL FEATURES TO EACH NODE\n",
    "            for name, fdict in textf.items():\n",
    "                nx.set_node_attributes(G_current, fdict, name)\n",
    "    \n",
    "    #CREATE PYG DATASET\n",
    "    if feature != 'constant' and feature != 'one-hot-encode':\n",
    "        group_attrs = all\n",
    "    else:\n",
    "        group_attrs = None\n",
    "\n",
    "        \n",
    "    current_data = from_networkx(G_current, group_node_attrs = group_attrs)\n",
    "    current_data.train_mask = current_data.val_mask = current_data.test_mask = current_data.y = None\n",
    "    \n",
    "    if feature == 'constant':\n",
    "        transform = Constant()\n",
    "        current_data = transform(current_data)\n",
    "    \n",
    "    if feature == 'one-hot-encode':\n",
    "        raise NotImplementedError('one-hot-encode not implemented')\n",
    "        \n",
    "    return current_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7602b28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_data = load_pyg_dataset(gpath_train, gpath_future, feature = 'topic', textfpath = textfpath_train, topicSim=True)\n",
    "\n",
    "#NORMALIZATION (L1-Norm)\n",
    "\n",
    "transform = NormalizeFeatures()\n",
    "current_data = transform(current_data)\n",
    "\n",
    "#TRAIN TEST SPLIT + NEGATIVE SAMPLING\n",
    "transform = RandomLinkSplit(num_val=0.0,num_test=0.25)\n",
    "train_data, val_data, current_test_data = transform(current_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfccd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_data = load_pyg_dataset(gpath_future, gpath_test_future, feature = 'topic', textfpath = textfpath_test,topicSim=True)\n",
    "\n",
    "#NORMALIZATION\n",
    "transform = NormalizeFeatures()\n",
    "future_data = transform(future_data)\n",
    "\n",
    "#NEGATIVE SAMPLING\n",
    "future_neg_edge_index = negative_sampling(\n",
    "        edge_index=future_data.edge_index, #positive edges\n",
    "        num_nodes=future_data.num_nodes, # number of nodes\n",
    "        num_neg_samples=future_data.edge_index.size(1)) # number of neg_sample equal to number of pos_edges\n",
    "\n",
    "#edge index ok, edge_label cat, edge_label_index cat\n",
    "num_pos_edge = future_data.edge_index.size(1)\n",
    "future_data.edge_label = torch.Tensor(np.array([1 for i in range(num_pos_edge)] + [0 for i in range(num_pos_edge)]))\n",
    "future_data.edge_label_index = torch.cat([future_data.edge_index, future_neg_edge_index], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb06308",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f110fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff32e30",
   "metadata": {},
   "source": [
    "# DATASET MANIPULATION UTILITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a909f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTrainTestNegEdgeIndex(dataset):\n",
    "    num_pos = len(dataset.edge_index[0])\n",
    "    neg_edge_index_src = dataset.edge_label_index[0][num_pos:]\n",
    "    neg_edge_index_trg = dataset.edge_label_index[1][num_pos:]\n",
    "    neg_edge_index = torch.Tensor(np.array([np.array(neg_edge_index_src),\\\n",
    "                                            np.array(neg_edge_index_trg)])).long()\n",
    "    return neg_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88723897",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getValNegEdgeIndex(dataset):\n",
    "    \n",
    "    def posNegSplitPoint(edge_label):\n",
    "        for i in range(1,len(edge_label)):\n",
    "            if edge_label[i-1] != edge_label[i]:\n",
    "                return i\n",
    "        return -1\n",
    "    \n",
    "    num_pos = posNegSplitPoint(dataset.edge_label)\n",
    "    neg_edge_index_src = dataset.edge_label_index[0][num_pos:]\n",
    "    neg_edge_index_trg = dataset.edge_label_index[1][num_pos:]\n",
    "    neg_edge_index = torch.Tensor(np.array([np.array(neg_edge_index_src),\\\n",
    "                                            np.array(neg_edge_index_trg)])).long()\n",
    "    return neg_edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a238d8fa",
   "metadata": {},
   "source": [
    "# GAE MODULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec194b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNEncoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNEncoder, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 2 * out_channels, cached=True) # cached only for transductive learning\n",
    "        self.conv2 = GCNConv(2 * out_channels, out_channels, cached=True) # cached only for transductive learning\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        return self.conv2(x, edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d2496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data):\n",
    "    x = data.x.float().to(device)\n",
    "    train_pos_edge_index = data.edge_index.to(device)\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z = model.encode(x, train_pos_edge_index)\n",
    "    loss = model.recon_loss(z, train_pos_edge_index)\n",
    "    #if args.variational:\n",
    "    #   loss = loss + (1 / data.num_nodes) * model.kl_loss()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "\n",
    "def test(data, pos_edge_index, neg_edge_index):\n",
    "    x = data.x.float().to(device)\n",
    "    current_pos_edge_index = data.edge_index.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(x, current_pos_edge_index)\n",
    "    return model.test(z, pos_edge_index, neg_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65380b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg_edge_index = getTrainTestNegEdgeIndex(train_data)\n",
    "val_pos_edge_index = current_test_data.edge_index\n",
    "val_neg_edge_index = getValNegEdgeIndex(current_test_data)\n",
    "test_pos_edge_index = future_data.edge_index\n",
    "test_neg_edge_index = future_neg_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e83e256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dd630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "out_channels = 2\n",
    "num_features = train_data.num_node_features\n",
    "\n",
    "# model\n",
    "model = GAE(GCNEncoder(num_features, out_channels))\n",
    "model.reset_parameters()\n",
    "\n",
    "# move to GPU (if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# inizialize the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cf3e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "best_epoch = 0\n",
    "ap_max = 0.0\n",
    "\n",
    "avgpr_trains = []\n",
    "avgpr_vals = []\n",
    "avgpr_tests = []\n",
    "\n",
    "best_model = copy.deepcopy(model)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    \n",
    "    loss = train(train_data)\n",
    "    \n",
    "    auc_train, ap_train = test(train_data, train_data.edge_index, train_neg_edge_index)\n",
    "    auc_val, ap_val = test(current_test_data, val_pos_edge_index, val_neg_edge_index)\n",
    "    auc_test, ap_test = test(future_data, test_pos_edge_index, test_neg_edge_index)\n",
    "    print('Epoch: {:03d}, AUC: train: {:.4f}, val: {:.4f}, test: {:.4f}, AP: train: {:.4f}, val: {:.4f}, test: {:.4f}'\\\n",
    "          .format(epoch, auc_train, auc_val, auc_test, ap_train, ap_val, ap_test))\n",
    "    \n",
    "    if ap_test >= ap_max:\n",
    "        best_epoch = epoch\n",
    "        ap_max = ap_test\n",
    "        best_model = copy.deepcopy(model)\n",
    "        \n",
    "    avgpr_trains.append(ap_train)\n",
    "    avgpr_vals.append(ap_val)\n",
    "    avgpr_tests.append(ap_test)\n",
    "\n",
    "\"\"\"\n",
    "#train orange test blue val green\n",
    "x = range(num_epochs)\n",
    "plt.clf()\n",
    "plt.plot(x, avgpr_trains, color='orange', label='avgpr_train')\n",
    "plt.plot(x, avgpr_vals, color='green', label='avgpr_val')\n",
    "plt.plot(x, avgpr_tests, color='blue', label = 'avgpr_test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AVGPR-score')\n",
    "plt.legend()\n",
    "plt.ylim(top=1)\n",
    "plt.grid()\n",
    "plt.savefig(f'learningCurves/GAE/august2016/new_all.pdf'\\\n",
    "            ,bbox_inches='tight')\n",
    "plt.clf()\n",
    "\"\"\"\n",
    "\n",
    "print(f'Best epoch: {best_epoch}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

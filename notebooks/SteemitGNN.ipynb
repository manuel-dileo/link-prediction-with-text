{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6b22489",
   "metadata": {},
   "source": [
    "To install pytorch geometric run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f7d42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch=='1.9.0'\n",
    "#!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric -f https://data.pyg.org/whl/torch-1.9.0+cu102.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76e04ce",
   "metadata": {},
   "source": [
    "# IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05d94c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score,average_precision_score\n",
    "\n",
    "from torch_geometric.utils import negative_sampling\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from torch_geometric.transforms import RandomLinkSplit,NormalizeFeatures,Constant,OneHotDegree\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.nn import GCNConv,SAGEConv,GATConv\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import torch\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import copy\n",
    "import itertools\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb8b40a",
   "metadata": {},
   "source": [
    "# LOAD DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b51e908",
   "metadata": {},
   "source": [
    "You have to choose only one of the following four cells to construct the time slices over a certain time interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cea13b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#time period used in the works\n",
    "time_slices = {\n",
    "    0: {\n",
    "        'future_start': '2016-07-03',\n",
    "        'future_end': '2016-08-02',\n",
    "        'start' : '2016-06-03',\n",
    "        'end': '2016-07-02'\n",
    "    },\n",
    "    \n",
    "    1: {\n",
    "        'future_start': '2016-08-03',\n",
    "        'future_end': '2016-09-02',\n",
    "        'start': '2016-07-03',\n",
    "        'end': '2016-08-02'\n",
    "    },\n",
    "    \n",
    "    2:{\n",
    "        'future_start': '2016-09-03',\n",
    "        'future_end': '2016-10-02',\n",
    "        'start': '2016-08-03',\n",
    "        'end': '2016-09-02'\n",
    "    },\n",
    "    \n",
    "    3:{\n",
    "        'future_start': '2016-10-03',\n",
    "        'future_end': '2016-11-02',\n",
    "        'start': '2016-09-03',\n",
    "        'end': '2016-10-02'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c49585",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 days time slice\n",
    "time_slices = {\n",
    "    0: {\n",
    "        'future_start': '2016-08-03',\n",
    "        'future_end': '2016-08-12',\n",
    "        'start' : '2016-06-03',\n",
    "        'end': '2016-08-02'\n",
    "    },\n",
    "    \n",
    "    1: {\n",
    "        'future_start': '2016-08-13',\n",
    "        'future_end': '2016-08-22',\n",
    "        'start': '2016-08-03',\n",
    "        'end': '2016-08-12'\n",
    "    },\n",
    "    \n",
    "    2:{\n",
    "        'future_start': '2016-08-23',\n",
    "        'future_end': '2016-09-01',\n",
    "        'start': '2016-08-13',\n",
    "        'end': '2016-08-22'\n",
    "    },\n",
    "    \n",
    "    3:{\n",
    "        'future_start': '2016-09-02',\n",
    "        'future_end': '2016-09-11',\n",
    "        'start': '2016-08-23',\n",
    "        'end': '2016-09-01'\n",
    "    },\n",
    "    \n",
    "    4:{\n",
    "        'future_start': '2016-09-12',\n",
    "        'future_end': '2016-09-21',\n",
    "        'start': '2016-09-02',\n",
    "        'end': '2016-09-11'\n",
    "    },\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f94e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nov-Dec 2016 time interval\n",
    "time_slices = {\n",
    "    0: {\n",
    "        'future_start': '2016-12-03',\n",
    "        'future_end': '2017-01-02',\n",
    "        'start' : '2016-06-03',\n",
    "        'end': '2016-12-02'\n",
    "    },\n",
    "    \n",
    "    1: {\n",
    "        'future_start': '2017-01-03',\n",
    "        'future_end': '2017-02-02',\n",
    "        'start': '2016-12-03',\n",
    "        'end': '2017-01-02'\n",
    "    },\n",
    "    \n",
    "    2:{\n",
    "        'future_start': '2017-02-03',\n",
    "        'future_end': '2017-03-02',\n",
    "        'start': '2017-01-03',\n",
    "        'end': '2017-02-02'\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e839d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nov-Dec 2016 time interval with 10 days time slice\n",
    "time_slices = {\n",
    "    0: {\n",
    "        'future_start': '2016-12-03',\n",
    "        'future_end': '2016-12-12',\n",
    "        'start' : '2016-06-03',\n",
    "        'end': '2016-12-02'\n",
    "    },\n",
    "    \n",
    "    1: {\n",
    "        'future_start': '2016-12-13',\n",
    "        'future_end': '2016-12-22',\n",
    "        'start': '2016-12-03',\n",
    "        'end': '2016-12-12'\n",
    "    },\n",
    "    \n",
    "    2:{\n",
    "        'future_start': '2016-12-23',\n",
    "        'future_end': '2017-01-01',\n",
    "        'start': '2016-12-13',\n",
    "        'end': '2016-12-22'\n",
    "    },\n",
    "    \n",
    "    3:{\n",
    "        'future_start': '2017-01-02',\n",
    "        'future_end': '2017-01-11',\n",
    "        'start': '2016-12-23',\n",
    "        'end': '2017-01-01'\n",
    "    },\n",
    "    4:{\n",
    "        'future_start': '2017-01-12',\n",
    "        'future_end': '2017-01-21',\n",
    "        'start': '2017-01-02',\n",
    "        'end': '2017-01-11'\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0ed7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PERIOD = 1\n",
    "\n",
    "start_train = time_slices[0]['start']\n",
    "end_train = time_slices[PERIOD]['end']\n",
    "\n",
    "start_future = time_slices[0]['start']\n",
    "end_future = time_slices[PERIOD]['future_end']\n",
    "\n",
    "start_future_test = time_slices[0]['start']\n",
    "end_future_test = time_slices[PERIOD+1]['future_end']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a2dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gpath_train = f\"dataset/graph/custom_json_{start_train}_{end_train}\"\n",
    "#gpath_future = f\"dataset/graph/custom_json_{start_future}_{end_future}\"\n",
    "#gpath_test_future = f\"dataset/graph/custom_json_{start_future_test}_{end_future_test}\"\n",
    "gpath_train = f\"../dummy-data/gnn/custom_json_{start_train}_{end_train}\"\n",
    "gpath_future = f\"../dummy-data/gnn/custom_json_{start_future}_{end_future}\"\n",
    "gpath_test_future = f\"../dummy-data/gnn/custom_json_{start_future_test}_{end_future_test}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4f0b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c7592a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#textfpath_train = f\"dataset/textFeatures/textFeatures_{start_train}_{end_train}.json\" \n",
    "#textfpath_test = f\"dataset/textFeatures/textFeatures_{start_future}_{end_future}.json\"\n",
    "textfpath_train = f\"../dummy-data/gnn/textFeatures_{start_train}_{end_train}.json\" \n",
    "textfpath_test = f\"../dummy-data/gnn/textFeatures_{start_future}_{end_future}.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3950bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#TO SEE NEW EDGES\n",
    "\n",
    "G_current_all = nx.read_edgelist(gpath_train, create_using=nx.DiGraph)\n",
    "print('Number of edges {}'.format(G_current_all.size()))\n",
    "G_future = nx.read_edgelist(gpath_future, create_using=nx.DiGraph)\n",
    "k = len(G_future.subgraph(G_current_all.nodes()).edges())\n",
    "print(\"New links {}\".format(k))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbfefc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pyg_dataset(gpath_current, gpath_future, feature = 'struct', textfpath = None, topicSim=False):\n",
    "    \n",
    "    if feature not in ['constant','one-hot-encode','struct','text','topic','all']:\n",
    "        raise Exception('Invalid feature value')\n",
    "    \n",
    "    #LOAD GRAPH AND COMPUTE NODE FEATURES\n",
    "    G_current_all = nx.read_edgelist(gpath_current, create_using=nx.DiGraph)\n",
    "\n",
    "    if feature == 'struct' or feature == 'all':\n",
    "        #COMPUTE STRUCTURAL FEATURES\n",
    "        pr = nx.pagerank(G_current_all)\n",
    "        indeg = dict(G_current_all.in_degree())\n",
    "        outdeg = dict(G_current_all.out_degree())\n",
    "        neighdeg = nx.average_neighbor_degree(G_current_all)\n",
    "    \n",
    "    if feature == 'text' or feature == 'topic' or feature == 'all':\n",
    "        #COMPUTE TEXTUAL FEATURES\n",
    "        if textfpath is None:\n",
    "            raise Exception('textfpath parameter must be not None if feature is text or all')\n",
    "        with open(textfpath) as f:\n",
    "            textf_raw = json.load(f)\n",
    "            textf = dict()\n",
    "            textf['numPost'] = textf_raw['numPost']\n",
    "            textf['numComment'] = textf_raw['numComment']\n",
    "            textf['numTag'] = textf_raw['numTag']\n",
    "            textf['corpusLength'] = textf_raw['corpusLength']\n",
    "            textf['corpusStdev'] = textf_raw['corpusStdev']\n",
    "            \n",
    "            if feature == 'topic':\n",
    "                topics = {}\n",
    "                for user,lda in textf_raw['lda_all'].items():\n",
    "                    for i in range(30): #30 is the number of LDA topic\n",
    "                        if f'topic_{i}' not in topics:\n",
    "                            topics[f'topic_{i}'] = {}\n",
    "                        topics[f'topic_{i}'][user] = lda[i]\n",
    "                        \n",
    "                for name,topicf in topics.items():\n",
    "                    textf[name] = topicf\n",
    "            \n",
    "            if topicSim: \n",
    "                #compute an index based on entropy of LDA distributions to reflect\n",
    "                #the user importance in term of its contents\n",
    "                #the study on this feature is still not covered on our works\n",
    "                entropy_all = {user:entropy(ldaAll) for user,ldaAll in textf_raw['lda_all'].items()}\n",
    "                #entropy_post = {user:entropy(ldaPost) for user,ldaPost in textf_raw['lda_post'].items()}\n",
    "                #entropy_comment = {user:entropy(ldaComment) for user,ldaComment in textf_raw['lda_comment'].items()}\n",
    "                reciprocal_mean_followers_entropy_all = {}\n",
    "                for user in G_current_all.nodes():\n",
    "                    sum_followers_entropies = 0\n",
    "                    count_followers = 0\n",
    "                    for follower in G_current_all.predecessors(user):\n",
    "                        follower_entropy = entropy_all[follower]\n",
    "                        sum_followers_entropies += follower_entropy\n",
    "                        count_followers += 1\n",
    "                    mean_followers_entropy = 0\n",
    "                    if count_followers > 0:\n",
    "                        mean_followers_entropy = sum_followers_entropies / count_followers\n",
    "                    reciprocal_mean = 0\n",
    "                    if mean_followers_entropy != 0:\n",
    "                        reciprocal_mean = 1 / mean_followers_entropy\n",
    "                    reciprocal_mean_followers_entropy_all[user] = reciprocal_mean\n",
    "                textf['reciprocal_mean_followers_entropy_all'] = reciprocal_mean_followers_entropy_all\n",
    "                    \n",
    "                \n",
    "    #POSITIVE SET WITH LABELLING ON THE FUTURE PERIOD\n",
    "    G_future = nx.read_edgelist(gpath_future, create_using=nx.DiGraph)\n",
    "\n",
    "    G_current = nx.DiGraph(G_future.subgraph(G_current_all.nodes()))\n",
    "    \n",
    "    if feature == 'struct' or feature == 'text' or feature == 'topic' or feature == 'all':\n",
    "        if feature == 'struct' or feature == 'all':\n",
    "            #ASSIGN STRUCTURAL FEATURES TO EACH NODE\n",
    "            nx.set_node_attributes(G_current,pr,'pagerank')\n",
    "            nx.set_node_attributes(G_current,indeg,'in_degree')\n",
    "            nx.set_node_attributes(G_current,outdeg,'out_degree')\n",
    "            nx.set_node_attributes(G_current,neighdeg,'neigh_degree')\n",
    "            #nx.set_node_attributes(G_train,nx.clustering(G_train),'clustering')\n",
    "            #nx.set_node_attributes(G_train,nx.katz_centrality(G_train),'katz')\n",
    "            \n",
    "        if feature == 'text' or feature == 'all' or feature == 'topic':\n",
    "            #ASSIGN TEXTUAL FEATURES TO EACH NODE\n",
    "            for name, fdict in textf.items():\n",
    "                nx.set_node_attributes(G_current, fdict, name)\n",
    "    \n",
    "    #CREATE PYG DATASET\n",
    "    if feature != 'constant' and feature != 'one-hot-encode':\n",
    "        group_attrs = all\n",
    "    else:\n",
    "        group_attrs = None\n",
    "\n",
    "        \n",
    "    current_data = from_networkx(G_current, group_node_attrs = group_attrs)\n",
    "    current_data.train_mask = current_data.val_mask = current_data.test_mask = current_data.y = None\n",
    "    \n",
    "    if feature == 'constant':\n",
    "        transform = Constant()\n",
    "        current_data = transform(current_data)\n",
    "    \n",
    "    if feature == 'one-hot-encode':\n",
    "        raise NotImplementedError('one-hot-encode not implemented')\n",
    "        \n",
    "    return current_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7602b28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_data = load_pyg_dataset(gpath_train, gpath_future, feature = 'topic', textfpath = textfpath_train)\n",
    "\n",
    "#NORMALIZATION (L1-Norm)\n",
    "\n",
    "transform = NormalizeFeatures()\n",
    "current_data = transform(current_data)\n",
    "\n",
    "#TRAIN TEST SPLIT + NEGATIVE SAMPLING\n",
    "transform = RandomLinkSplit(num_val=0.0,num_test=0.25)\n",
    "train_data, val_data, current_test_data = transform(current_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfccd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_data = load_pyg_dataset(gpath_future, gpath_test_future, feature = 'topic', textfpath = textfpath_test)\n",
    "\n",
    "#NORMALIZATION\n",
    "transform = NormalizeFeatures()\n",
    "future_data = transform(future_data)\n",
    "\n",
    "#NEGATIVE SAMPLING\n",
    "future_neg_edge_index = negative_sampling(\n",
    "        edge_index=future_data.edge_index, #positive edges\n",
    "        num_nodes=future_data.num_nodes, # number of nodes\n",
    "        num_neg_samples=future_data.edge_index.size(1)) # number of neg_sample equal to number of pos_edges\n",
    "\n",
    "#edge index ok, edge_label concat, edge_label_index concat\n",
    "num_pos_edge = future_data.edge_index.size(1)\n",
    "future_data.edge_label = torch.Tensor(np.array([1 for i in range(num_pos_edge)] + [0 for i in range(num_pos_edge)]))\n",
    "future_data.edge_label_index = torch.cat([future_data.edge_index, future_neg_edge_index], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb06308",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f110fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3597e0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MY GNN CUSTOM MODULE\n",
    "class LinkPredModel(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, layer = GCNConv, dropout=0.25, loss = torch.nn.BCEWithLogitsLoss):\n",
    "        \n",
    "        super(LinkPredModel, self).__init__()\n",
    "        self.conv1 = layer(input_dim, hidden_dim)\n",
    "        self.conv2 = layer(hidden_dim, num_classes)\n",
    "        \n",
    "        #Initialize the loss function to BCEWithLogitsLoss\n",
    "        self.loss_fn = loss()\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.conv1.reset_parameters()\n",
    "        self.conv2.reset_parameters()\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x , edge_index, edge_label_index = batch.x.float(), batch.edge_index, batch.edge_label_index\n",
    "        \n",
    "        ## Note\n",
    "        ## 1. Feed the node feature into the first conv layer\n",
    "        ## 2. Add a leaky-ReLU after the first conv layer\n",
    "        ## 3. Add dropout after the ReLU (with probability self.dropout)\n",
    "        ## 4. Repeat for the next layers\n",
    "        ## 5. Select the embeddings of the source nodes and destination nodes\n",
    "        ## by using the edge_label_index and compute the similarity of each pair\n",
    "        ## by dot product\n",
    "\n",
    "        h = self.conv1(x, batch.edge_index)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = F.dropout(h, p=self.dropout)\n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = F.leaky_relu(h)\n",
    "        h = F.dropout(h, p=self.dropout)\n",
    "        \n",
    "        h_src = h[edge_label_index[0]]\n",
    "        h_dst = h[edge_label_index[1]]\n",
    "        h_sim = h_src * h_dst #dot product\n",
    "        pred = torch.sum(h_sim, dim=-1)\n",
    "\n",
    "        return pred\n",
    "    \n",
    "    def loss(self, pred, link_label):\n",
    "        return self.loss_fn(pred, link_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c87220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "def train(model, train_data, val_data, test_data, device,\\\n",
    "          optimizer, num_epochs=200, verbose=True):\n",
    "    \n",
    "    avgpr_val_max = 0\n",
    "    best_model = copy.deepcopy(model)\n",
    "    train_data = train_data.to(device)\n",
    "    best_epoch = -1\n",
    "    \n",
    "    avgpr_trains = []\n",
    "    avgpr_vals = []\n",
    "    avgpr_tests = []\n",
    "    \n",
    "    #roc_trains = []\n",
    "    #roc_vals = []\n",
    "    #roc_tests = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        ## Note\n",
    "        ## 1. Zero grad the optimizer\n",
    "        ## 2. Compute loss and backpropagate\n",
    "        ## 3. Update the model parameters\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        #pred = best_model(train_data)\n",
    "\n",
    "        pred = model(train_data)\n",
    "        loss = model.loss(pred, train_data.edge_label.type_as(pred))\n",
    "\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "        log = 'Epoch: {:03d}\\n AVGPR Train: {:.4f}, Val: {:.4f}, Test: {:.4f}\\n ROC Train: {:.4f}, Val: {:.4f}, Test: {:.4f}\\n F1-Score Train: {:.4f}, Val: {:.4f}, Test: {:.4f}\\n Loss: {}'\n",
    "        avgpr_score_train, f1_score_train, roc_score_train = test(model, train_data, device)\n",
    "        avgpr_score_val, f1_score_val, roc_score_val = test(model, val_data, device)\n",
    "        avgpr_score_test, f1_score_test, roc_score_test = test(model, test_data, device)\n",
    "        #score_test = test(model, dataloaders['test'], args)\n",
    "        \n",
    "        #f1_trains.append(f1_score_train)\n",
    "        #f1_vals.append(f1_score_val)\n",
    "        #f1_tests.append(f1_score_test)\n",
    "        \n",
    "        avgpr_trains.append(avgpr_score_train)\n",
    "        avgpr_vals.append(avgpr_score_val)\n",
    "        avgpr_tests.append(avgpr_score_test)\n",
    "        \n",
    "        if verbose:\n",
    "            print(log.format(epoch, avgpr_score_train, avgpr_score_val, avgpr_score_test, roc_score_train, roc_score_val, roc_score_test, f1_score_train, f1_score_val, f1_score_test, loss.item()))\n",
    "            \n",
    "        if avgpr_val_max < avgpr_score_val:\n",
    "            avgpr_val_max = avgpr_score_val\n",
    "            best_epoch = epoch\n",
    "            best_model = copy.deepcopy(model)\n",
    "            \n",
    "    if verbose:\n",
    "        print(f'Best Epoch: {best_epoch}')\n",
    "        \n",
    "    return best_model, avgpr_trains, avgpr_vals, avgpr_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79535ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_data, device):\n",
    "    model.eval()\n",
    "    \n",
    "    f1_model_score=0\n",
    "\n",
    "    test_data = test_data.to(device)\n",
    "\n",
    "    h = model(test_data)\n",
    "    \n",
    "    pred_cont = torch.sigmoid(h).cpu().detach().numpy()\n",
    "    pred = [1 if p > 0.5 else 0 for p in pred_cont]\n",
    "\n",
    "    label = test_data.edge_label.cpu().detach().numpy()\n",
    "      \n",
    "    roc_score = roc_auc_score(label, pred_cont)\n",
    "    avgpr_score = average_precision_score(label, pred_cont)\n",
    "    f1_model_score = f1_score(label,pred)\n",
    " \n",
    "    return avgpr_score, f1_model_score, roc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437a2581",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = train_data.num_node_features\n",
    "num_classes = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b296ee6",
   "metadata": {},
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ea6d54",
   "metadata": {},
   "source": [
    "Run this part of the notebook only if you want to re-perform hyperparameter tuning. Below you will find the model with the best config of hypeparameters found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e17870",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_all = {\n",
    "    'optimizer': ['sgd','adam','adagrad'],\n",
    "    'hidden_dim': [4,8,16, 32, 64, 128, 256],\n",
    "    'layer': [GATConv, GCNConv, SAGEConv],\n",
    "    'dropout': [0,0.25,0.50,0.75],\n",
    "    'weight_decay': [5e-5,5e-3,5e-2,5e-1,5e-6],\n",
    "    'lr': [1,0.1,0.075,0.05,0.025,0.01,0.001,0.0001,0.00001]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5ae1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_1 = {\n",
    "    'optimizer': ['sgd','adam','adagrad'],\n",
    "    'hidden_dim': [16, 32, 64, 128, 256],\n",
    "    'layer': [GATConv, GCNConv, SAGEConv],\n",
    "    'dropout': [0.25,0.50,0.75],\n",
    "    'lr': [1,0.1,0.01,0.001,0.0001,0.00001]\n",
    "}\n",
    "\n",
    "## USATA PER PRIMI EXP (STRUCT F PERIOD 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17ea9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_2 = {\n",
    "    'optimizer': ['sgd','adam','adagrad'],\n",
    "    'hidden_dim': [16,32],\n",
    "    'layer': [GATConv, GCNConv, SAGEConv],\n",
    "    'dropout': [0.0, 0.25],\n",
    "    'lr': [0.1,0.01,0.001]\n",
    "}\n",
    "\n",
    "## USATA PER CONSTANT PERIOD 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4922358",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_3 = {\n",
    "    'optimizer': ['adam'],\n",
    "    'hidden_dim': [16,32],\n",
    "    'layer': [GCNConv],\n",
    "    'dropout': [0.0],\n",
    "    'lr': [0.1,0.01]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93070892",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_4 = {\n",
    "    'optimizer': ['adam'],\n",
    "    'hidden_dim': [4,8],\n",
    "    'layer': [GCNConv],\n",
    "    'dropout': [0.0],\n",
    "    'lr': [0.025,0.05,0.075],\n",
    "    'weight_decay': [5e-5,5e-3,5e-2,5e-1,5e-6]\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f578f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gnn_tuning(grid, input_dim, num_classes, device, train_data, current_test_data, test_data, logname, fold, plotF1=True):\n",
    "    names, values = zip(*grid.items())\n",
    "    configurations = [dict(zip(names,v)) for v in itertools.product(*values)]\n",
    "    print('There are ', len(configurations), ' configurations for this model')\n",
    "    \n",
    "    i=0\n",
    "    avgpr_max = 0\n",
    "    best_config = {}\n",
    "    best_config_i = 0\n",
    "    weight_decay = 5e-4\n",
    "    with open(logname,'a+') as log:\n",
    "        for config in configurations:\n",
    "            print('Configuration number ', str(i), ' start')\n",
    "            \n",
    "            log.write('CONFIGURATION ' + str(i) + '\\n\\n')\n",
    "            writeConfig = copy.deepcopy(config)\n",
    "            writeConfig['layer'] = writeConfig['layer'].__name__\n",
    "            nameLayer = writeConfig['layer']\n",
    "            log.write(json.dumps(writeConfig,indent=2))\n",
    "            log.write('\\n')\n",
    "\n",
    "            hidden_dim = config['hidden_dim']\n",
    "            layer = config['layer']\n",
    "            dropout = config['dropout']\n",
    "            if 'weight_decay' in config:\n",
    "                weight_decay = config['weight_decay']\n",
    "            model = LinkPredModel(input_dim, hidden_dim, num_classes, dropout = dropout, layer = layer).to(device)\n",
    "            model.reset_parameters()\n",
    "            \n",
    "            opt = config['optimizer']\n",
    "            lr = config['lr']\n",
    "            if opt == 'sgd':\n",
    "                optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
    "            elif opt == 'adam':\n",
    "                optimizer = torch.optim.Adam(params=model.parameters(), lr=lr, weight_decay = weight_decay)\n",
    "            else:\n",
    "                optimizer = torch.optim.Adagrad(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "            best_model, avgpr_trains, avgpr_vals, avgpr_tests = \\\n",
    "            train(model, train_data, current_test_data, test_data, device, optimizer,verbose=False)\n",
    "            \n",
    "            print('GNN fitted')\n",
    "\n",
    "            avgpr_train, f1_score_train, roc_score_train = test(best_model, train_data, device)\n",
    "            avgpr_val, f1_score_current_test, roc_score_val = test(best_model, current_test_data, device)\n",
    "            avgpr_test, f1_score_future_test, roc_score_test = test(best_model, future_data, device)\n",
    "            \n",
    "            print('GNN performance computed')\n",
    "\n",
    "            log.write('TRAIN SET\\n')\n",
    "            log.write(f' AVGPR SCORE: {avgpr_train}\\n')\n",
    "            log.write(f' ROC SCORE: {roc_score_train}\\n')\n",
    "            log.write(f' F1-SCORE: {f1_score_train}\\n\\n')\n",
    "\n",
    "            log.write('CURRENT TEST SET\\n')\n",
    "            log.write(f' AVGPR SCORE: {avgpr_val}\\n')\n",
    "            log.write(f' ROC SCORE: {roc_score_val}\\n')\n",
    "            log.write(f' F1-SCORE: {f1_score_current_test}\\n\\n')\n",
    "\n",
    "            log.write('FUTURE TEST SET\\n')\n",
    "            log.write(f' AVGPR SCORE: {avgpr_test}\\n')\n",
    "            log.write(f' ROC SCORE: {roc_score_test}\\n')\n",
    "            log.write(f' F1-SCORE: {f1_score_future_test}\\n')\n",
    "            \n",
    "            print('GNN performance written')\n",
    "\n",
    "            if avgpr_max < avgpr_test:\n",
    "                avgpr_max = avgpr_test\n",
    "                best_config = config\n",
    "                best_config_i = i\n",
    "                scores = {\n",
    "                    'avgpr_train': avgpr_train,\n",
    "                    'roc_score_train': roc_score_train,\n",
    "                    'f1_score_train': f1_score_train,\n",
    "                    'avgpr_current_test': avgpr_val,\n",
    "                    'roc_score_val': roc_score_val,\n",
    "                    'f1_score_current_test': f1_score_current_test,\n",
    "                    'avgpr_future_test': avgpr_test,\n",
    "                    'roc_score_test': roc_score_test,\n",
    "                    'f1_score_future_test': f1_score_future_test\n",
    "                }\n",
    "                top_model = copy.deepcopy(best_model)\n",
    "                \n",
    "            if plotF1:\n",
    "                #train orange test blue val green\n",
    "                num_epochs = 200\n",
    "                x = range(num_epochs)\n",
    "                plt.clf()\n",
    "                plt.plot(x, avgpr_trains, color='orange', label='avgpr_train')\n",
    "                plt.plot(x, avgpr_vals, color='green', label='avgpr_val')\n",
    "                plt.plot(x, avgpr_tests, color='blue', label = 'avgpr_test')\n",
    "                plt.xlabel('Epoch')\n",
    "                plt.ylabel('AVGPR-score')\n",
    "                plt.legend()\n",
    "                plt.ylim(top=1)\n",
    "                plt.grid()\n",
    "                plt.savefig(f'learningCurves/{fold}/{nameLayer}_{hidden_dim}_{dropout}_{opt}_{lr}_{weight_decay}.pdf'\\\n",
    "                            ,bbox_inches='tight')\n",
    "                plt.clf()\n",
    "            \n",
    "            print('Configuration number ', str(i), ' end')\n",
    "            print()\n",
    "            \n",
    "            log.write('\\n\\n')\n",
    "            log.flush()\n",
    "\n",
    "            i+=1\n",
    "            \n",
    "        log.write(f'Best configuration number: {best_config_i}\\n')\n",
    "        best_config_write = copy.deepcopy(best_config)\n",
    "        best_config_write['layer'] = best_config_write['layer'].__name__\n",
    "        log.write(json.dumps(best_config_write,indent=2))\n",
    "        log.write('\\n')\n",
    "        log.write(json.dumps(scores,indent=2))\n",
    "        log.write('\\n')\n",
    "        log.flush()\n",
    "        log.close()\n",
    "        \n",
    "    return top_model, best_config, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab5cfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, best_config, scores = gnn_tuning(grid_4, input_dim, num_classes, device, train_data, current_test_data, future_data, 'new_grid_gnn_text_august2016.txt', 'GNN/new_grid/text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd84cad",
   "metadata": {},
   "source": [
    "## Single Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bed96eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinkPredModel(input_dim, 16, num_classes, dropout=0.0, layer=GCNConv).to(device)\n",
    "model.reset_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfebad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 5e-4\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=weight_decay)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01, weight_decay = weight_decay)\n",
    "#optimizer = torch.optim.RMSprop(model.parameters(), lr=0.1, weight_decay = weight_decay)\n",
    "#optimizer = torch.optim.Adagrad(model.parameters(), lr=0.01, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee3d40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model , prtrain , prval, prtest = train(model, train_data, current_test_data, future_data, device, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d329ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train orange test blue val green\n",
    "num_epochs = 200\n",
    "x = range(num_epochs)\n",
    "plt.clf()\n",
    "plt.plot(x, prtrain, color='orange', label='avgpr_train')\n",
    "plt.plot(x, prval, color='green', label='avgpr_val')\n",
    "plt.plot(x, prtest, color='blue', label = 'avgpr_test')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AVGPR-score')\n",
    "plt.legend()\n",
    "plt.ylim(top=1)\n",
    "plt.grid()\n",
    "plt.savefig(f'new_AVGPR_learningCurve_GNNAll_august2016.pdf',bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25793224",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_train, f1_score_train = test(best_model, train_data, device)\n",
    "roc_val, f1_score_current_test = test(best_model, current_test_data, device)\n",
    "roc_test, f1_score_future_test = test(best_model, future_data, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5500cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('TRAIN SET')\n",
    "print(f' ROC AUC SCORE: {roc_train}')\n",
    "print(f' F1-SCORE: {f1_score_train}')\n",
    "\n",
    "print()\n",
    "\n",
    "print('CURRENT TEST SET')\n",
    "print(f' ROC AUC SCORE: {roc_val}')\n",
    "print(f' F1-SCORE: {f1_score_current_test}')\n",
    "\n",
    "print()\n",
    "\n",
    "print('FUTURE TEST SET')\n",
    "print(f' ROC AUC SCORE: {roc_test}')\n",
    "print(f' F1-SCORE: {f1_score_future_test}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
